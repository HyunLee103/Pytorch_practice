{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "timbreTron.ipynb",
      "provenance": [],
      "mount_file_id": "1MBDUXu2Vo-4JX_uuJFV6GBthP8lpVuWq",
      "authorship_tag": "ABX9TyNGo8pVjwYqa8G7+pIhyrNZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyunLee103/Pytorch_practice/blob/master/timbreTron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX_WAaSglHr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "import json\n",
        "\n",
        "data_dir = '/content/drive/Shared drives/Music_Style_Transform/json_sample'\n",
        "save_dir_b = '/content/drive/My Drive/ADV_Project_Music_style_transform/cqt_sample/bass'\n",
        "save_dir_d = '/content/drive/My Drive/ADV_Project_Music_style_transform/cqt_sample/drum'\n",
        "save_dir_o = '/content/drive/My Drive/ADV_Project_Music_style_transform/cqt_sample/other'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkl44brFo4kt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "outputId": "001ad08b-f5a3-48b3-e771-98a7702a79dc"
      },
      "source": [
        "for i,(dirpath,dirnames,filenames) in enumerate(os.walk(data_dir)):\n",
        "    if dirpath is not data_dir:\n",
        "\n",
        "        genre = dirpath.split('/')[-1]\n",
        "        print(\"\\n Processing : {}\".format(genre))\n",
        "        \n",
        "        for index, f in enumerate(filenames):\n",
        "            file_path = os.path.join(dirpath,f)\n",
        "            id = f[1:5]\n",
        "            seg = f[6]\n",
        "            \n",
        "            print(index, f)\n",
        "\n",
        "            with open(file_path) as fp:\n",
        "                data = json.load(fp)\n",
        "                try:\n",
        "                    data = np.array(data)\n",
        "                    data = data.reshape(480000)\n",
        "                    all = np.abs(librosa.core.cqt(data, 32000))\n",
        "                    all = all[:,:,np.newaxis]\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "\n",
        "                if index % 2 != 0:\n",
        "                    with open(file_path) as fp:\n",
        "                        data = json.load(fp)\n",
        "                        data = np.array(data['wave'])\n",
        "                        for i in range(3):\n",
        "                            if i == 0:\n",
        "                                drum = data[i]\n",
        "                                drum = drum.reshape(480000)\n",
        "                                drum = np.abs(librosa.core.cqt(drum, 32000))\n",
        "                                drum = drum[:,:,np.newaxis]\n",
        "                                da = np.concatenate((drum,all),axis=2)\n",
        "                                np.save(os.path.join(save_dir_d,'{}_{}_da_{}_cqt'.format(genre,id,seg)),da)\n",
        "                            elif i == 1:\n",
        "                                bass = data[i]\n",
        "                                bass = bass.reshape(480000)\n",
        "                                bass = np.abs(librosa.core.cqt(bass, 32000))\n",
        "                                bass = bass[:,:,np.newaxis]\n",
        "                                ba = np.concatenate((bass,all),axis=2)\n",
        "                                np.save(os.path.join(save_dir_b,'{}_{}_ba_{}_cqt'.format(genre,id,seg)),ba)\n",
        "                            elif i == 2:\n",
        "                                other = data[i]\n",
        "                                other = other.reshape(480000)\n",
        "                                other = np.abs(librosa.core.cqt(other, 32000))\n",
        "                                other = other[:,:,np.newaxis]\n",
        "                                oa = np.concatenate((other,all),axis=2)\n",
        "                                np.save(os.path.join(save_dir_o,'{}_{}_oa_{}_cqt'.format(genre,id,seg)),oa)\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Processing : jazz\n",
            "0 00237_0_all.json\n",
            "1 00237_0.json\n",
            "2 00237_1_all.json\n",
            "3 00237_1.json\n",
            "4 00590_0_all.json\n",
            "5 00590_0.json\n",
            "6 00590_1_all.json\n",
            "7 00590_1.json\n",
            "8 00591_0_all.json\n",
            "9 00591_0.json\n",
            "10 00591_1_all.json\n",
            "11 00591_1.json\n",
            "\n",
            " Processing : pop\n",
            "0 04064_0_all.json\n",
            "1 04064_0.json\n",
            "2 04064_1_all.json\n",
            "3 04064_1.json\n",
            "4 04065_0_all.json\n",
            "5 04065_0.json\n",
            "6 04065_1_all.json\n",
            "7 04065_1.json\n",
            "8 04066_0_all.json\n",
            "9 04066_0.json\n",
            "10 04066_1_all.json\n",
            "11 04066_1.json\n",
            "12 04067_0_all.json\n",
            "13 04067_0.json\n",
            "14 04067_1_all.json\n",
            "15 04067_1.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuvkplOnlxs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing_data(path):\n",
        "    for i,(dirpath,dirnames,filenames) in enumerate(os.walk(data_dir)):\n",
        "        if dirpath is not data_dir:\n",
        "\n",
        "            genre = dirpath.split('/')[-1]\n",
        "            print(\"\\n Processing : {}\".format(genre_label))\n",
        "\n",
        "            for f in filenames:\n",
        "                file_path = os.path.join(dirpath,f)\n",
        "                \n",
        "\n",
        "\n",
        "    lst_data = os.listdir(data_dir)\n",
        "    lst_data = [f for f in lst_data if f.endswith('all')]\n",
        "    with open(data_path, \"r\") as fp:\n",
        "        data = json.load(fp)\n",
        "  \n",
        "        a = np.array(data['mfcc'])\n",
        "        b = np.array(data['labels'])\n",
        "\n",
        "        print(\"데이터 load 성공!\")\n",
        "\n",
        "        return a, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2t7RxKkdVs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "j_237_1 = np.array(json_data['wave'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-xpYLdfCj5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(j_237_1.shape[0]):\n",
        "    if i == 0:\n",
        "        j_237_d_1 = j_237_1[i]\n",
        "    elif i == 1:\n",
        "        j_237_b_1 = j_237_1[i]\n",
        "    elif i == 2:\n",
        "        j_237_o_1 = j_237_1[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XgX_xjheiJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wav2cqt(x, y):    \n",
        "    x = x.reshape(480000)\n",
        "    x = np.abs(librosa.core.cqt(x, 32000))\n",
        "    x = x[:,:,np.newaxis]\n",
        "    x = np.concatenate((x,j_237_all_1_cqt),axis=2)\n",
        "    print(x.shape)\n",
        "\n",
        "    np.save(os.path.join(data_dir,'j_237_{}a_1_cqt'.format(y)),x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh8bUevwicz1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "8d90cada-3a4c-432b-d158-8e2f90421fc7"
      },
      "source": [
        "wav2cqt(j_237_d_1 , 'd')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(84, 938, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEdDVjoDj8mF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def all2cqt(x):\n",
        "    x = x.reshape(480000)\n",
        "    x = np.abs(librosa.core.cqt(x, 32000))\n",
        "    x = x[:,:,np.newaxis]\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj0Cno1riNoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "j_237_all_1_cqt = all2cqt(j_237_all_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUwpbGQ3hP2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8rsnLU6kbW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX0qy2j_kbUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-EoLljvkbRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AlwWWuzfAJv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "a4ce1c7f-c733-4c0e-9c2b-7b8b008ecba0"
      },
      "source": [
        "wav2cqt(p_4064_0_all)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object wav2cqt at 0x7f59c0a8f0a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSJrS3Vvf32M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_4064_all_0_cqt = out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwsIECOQgKR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_4064_all_0_cqt = p_4064_all_0_cqt[:,:,np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZGAx-gHgVMP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "cfa9931d-caaa-40d2-a667-22b6e427c45e"
      },
      "source": [
        "p_4064_all_0_cqt.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(84, 938, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg02l5XH8ObX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "j_237_0 = j_237"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytzXQTLh8WLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "j_237_d_0 = j_237[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX3oDdYl9g2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "j_237_b_0 = j_237[1]\n",
        "j_237_o_0 = j_237[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYMqzL9kDLks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tribNeV_DeRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "j_237_0_cqt = librosa.core.cqt(j_237_0,32000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLDk_-4hDqkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "j_237_0 = j_237_0.reshape(480000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1fPFwJ1D4xm",
        "colab_type": "code",
        "outputId": "83e4a398-83a3-4b6a-dc7f-836cd667b159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "j_237_0.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(480000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anMtMkbGpJAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90CFIOkGQBOR",
        "colab_type": "code",
        "outputId": "9004ab28-1774-4555-8998-38fdeebcb76d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "x = j_237_all_0.reshape(480000)\n",
        "out = np.abs(librosa.core.cqt(x, 32000))\n",
        "j_237_all_0_cqt = out\n",
        "print(out.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(84, 938)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZQ9daIaMIun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [j_237_d_0_cqt,j_237_b_0_cqt,j_237_o_0_cqt,j_237_all_0_cqt,p_4064_d_1_cqt,p_4064_b_1_cqt,p_4064_o_1_cqt,p_4064_all_1_cqt]  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL2JfOI6RRtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in data:\n",
        "    librosa.display.specshow(librosa.amplitude_to_db(i, ref=np.max),sr=32000, x_axis='time', y_axis='cqt_note')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('CQT')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1vBbyOseb5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_4064_o_1_cqt = p_4064_o_1_cqt[:,:,np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgV00xCCeR4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_4064_oa_1_cqt = np.concatenate((p_4064_o_1_cqt,p_4064_all_1_cqt),axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH4IrOn1fgw1",
        "colab_type": "code",
        "outputId": "d7be77d3-32a7-4840-9c68-5f80ce946f0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "j_237_oa_0_cqt.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(84, 938, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 280
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff-Xh7acgVKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcUHJD7YVTcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(os.path.join(result_dir_train,'p_4064_oa_1_cqt'),np.abs(p_4064_oa_1_cqt))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLk9CypQRq0N",
        "colab_type": "text"
      },
      "source": [
        "# shape : (84, 938)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIv02MerUbsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        conv_block = [  nn.ReflectionPad2d(1),\n",
        "                        nn.Conv2d(in_features, in_features, 3),\n",
        "                        nn.InstanceNorm2d(in_features),\n",
        "                        nn.ReLU(inplace=True),\n",
        "                        nn.ReflectionPad2d(1),\n",
        "                        nn.Conv2d(in_features, in_features, 3),\n",
        "                        nn.InstanceNorm2d(in_features)  ]\n",
        "\n",
        "        self.conv_block = nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # Initial convolution block       \n",
        "        model = [   nn.ReflectionPad2d(3),\n",
        "                    nn.Conv2d(input_nc, 64, 7),\n",
        "                    nn.InstanceNorm2d(64),\n",
        "                    nn.ReLU(inplace=True) ]\n",
        "\n",
        "        # Downsampling\n",
        "        in_features = 64\n",
        "        out_features = in_features*2\n",
        "        for _ in range(2):\n",
        "            model += [  nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
        "                        nn.InstanceNorm2d(out_features),\n",
        "                        nn.ReLU(inplace=True) ]\n",
        "            in_features = out_features\n",
        "            out_features = in_features*2\n",
        "\n",
        "        # Residual blocks\n",
        "        for _ in range(n_residual_blocks):\n",
        "            model += [ResidualBlock(in_features)]\n",
        "\n",
        "        # Upsampling\n",
        "        out_features = in_features//2\n",
        "        for _ in range(2):\n",
        "            model += [  nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
        "                        nn.InstanceNorm2d(out_features),\n",
        "                        nn.ReLU(inplace=True) ]\n",
        "            in_features = out_features\n",
        "            out_features = in_features//2\n",
        "\n",
        "        # Output layer\n",
        "        model += [  nn.ReflectionPad2d(3),\n",
        "                    nn.Conv2d(64, output_nc, 7),\n",
        "                    nn.Tanh() ]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_nc):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # A bunch of convolutions one after another\n",
        "        model = [   nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n",
        "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
        "\n",
        "        model += [  nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
        "                    nn.InstanceNorm2d(128), \n",
        "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
        "\n",
        "        model += [  nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
        "                    nn.InstanceNorm2d(256), \n",
        "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
        "\n",
        "        model += [  nn.Conv2d(256, 512, 4, padding=1),\n",
        "                    nn.InstanceNorm2d(512), \n",
        "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
        "\n",
        "        # FCN classification layer\n",
        "        model += [nn.Conv2d(512, 1, 4, padding=1)]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x =  self.model(x)\n",
        "        # Average pooling and flatten\n",
        "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGdfIGWfVAv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install visdom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAt45g2FU6dV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import sys\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "from visdom import Visdom\n",
        "import numpy as np\n",
        "\n",
        "def tensor2image(tensor):\n",
        "    image = 127.5*(tensor[0].cpu().float().numpy() + 1.0)\n",
        "    if image.shape[0] == 1:\n",
        "        image = np.tile(image, (3,1,1))\n",
        "    return image.astype(np.uint8)\n",
        "\n",
        "class Logger():\n",
        "    def __init__(self, n_epochs, batches_epoch):\n",
        "        self.viz = Visdom()\n",
        "        self.n_epochs = n_epochs\n",
        "        self.batches_epoch = batches_epoch\n",
        "        self.epoch = 1\n",
        "        self.batch = 1\n",
        "        self.prev_time = time.time()\n",
        "        self.mean_period = 0\n",
        "        self.losses = {}\n",
        "        self.loss_windows = {}\n",
        "        self.image_windows = {}\n",
        "\n",
        "\n",
        "    def log(self, losses=None, images=None):\n",
        "        self.mean_period += (time.time() - self.prev_time)\n",
        "        self.prev_time = time.time()\n",
        "\n",
        "        sys.stdout.write('\\rEpoch %03d/%03d [%04d/%04d] -- ' % (self.epoch, self.n_epochs, self.batch, self.batches_epoch))\n",
        "\n",
        "        for i, loss_name in enumerate(losses.keys()):\n",
        "            if loss_name not in self.losses:\n",
        "                self.losses[loss_name] = losses[loss_name].data[0]\n",
        "            else:\n",
        "                self.losses[loss_name] += losses[loss_name].data[0]\n",
        "\n",
        "            if (i+1) == len(losses.keys()):\n",
        "                sys.stdout.write('%s: %.4f -- ' % (loss_name, self.losses[loss_name]/self.batch))\n",
        "            else:\n",
        "                sys.stdout.write('%s: %.4f | ' % (loss_name, self.losses[loss_name]/self.batch))\n",
        "\n",
        "        batches_done = self.batches_epoch*(self.epoch - 1) + self.batch\n",
        "        batches_left = self.batches_epoch*(self.n_epochs - self.epoch) + self.batches_epoch - self.batch \n",
        "        sys.stdout.write('ETA: %s' % (datetime.timedelta(seconds=batches_left*self.mean_period/batches_done)))\n",
        "\n",
        "        # Draw images\n",
        "        for image_name, tensor in images.items():\n",
        "            if image_name not in self.image_windows:\n",
        "                self.image_windows[image_name] = self.viz.image(tensor2image(tensor.data), opts={'title':image_name})\n",
        "            else:\n",
        "                self.viz.image(tensor2image(tensor.data), win=self.image_windows[image_name], opts={'title':image_name})\n",
        "\n",
        "        # End of epoch\n",
        "        if (self.batch % self.batches_epoch) == 0:\n",
        "            # Plot losses\n",
        "            for loss_name, loss in self.losses.items():\n",
        "                if loss_name not in self.loss_windows:\n",
        "                    self.loss_windows[loss_name] = self.viz.line(X=np.array([self.epoch]), Y=np.array([loss/self.batch]), \n",
        "                                                                    opts={'xlabel': 'epochs', 'ylabel': loss_name, 'title': loss_name})\n",
        "                else:\n",
        "                    self.viz.line(X=np.array([self.epoch]), Y=np.array([loss/self.batch]), win=self.loss_windows[loss_name], update='append')\n",
        "                # Reset losses for next epoch\n",
        "                self.losses[loss_name] = 0.0\n",
        "\n",
        "            self.epoch += 1\n",
        "            self.batch = 1\n",
        "            sys.stdout.write('\\n')\n",
        "        else:\n",
        "            self.batch += 1\n",
        "\n",
        "        \n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, max_size=50):\n",
        "        assert (max_size > 0), 'Empty buffer or trying to create a black hole. Be careful.'\n",
        "        self.max_size = max_size\n",
        "        self.data = []\n",
        "\n",
        "    def push_and_pop(self, data):\n",
        "        to_return = []\n",
        "        for element in data.data:\n",
        "            element = torch.unsqueeze(element, 0)\n",
        "            if len(self.data) < self.max_size:\n",
        "                self.data.append(element)\n",
        "                to_return.append(element)\n",
        "            else:\n",
        "                if random.uniform(0,1) > 0.5:\n",
        "                    i = random.randint(0, self.max_size-1)\n",
        "                    to_return.append(self.data[i].clone())\n",
        "                    self.data[i] = element\n",
        "                else:\n",
        "                    to_return.append(element)\n",
        "        return Variable(torch.cat(to_return))\n",
        "\n",
        "class LambdaLR():\n",
        "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
        "        assert ((n_epochs - decay_start_epoch) > 0), \"Decay must start before the training session ends!\"\n",
        "        self.n_epochs = n_epochs\n",
        "        self.offset = offset\n",
        "        self.decay_start_epoch = decay_start_epoch\n",
        "\n",
        "    def step(self, epoch):\n",
        "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch)/(self.n_epochs - self.decay_start_epoch)\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.normal(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm2d') != -1:\n",
        "        torch.nn.init.normal(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant(m.bias.data, 0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vio0QIoU992",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import random\n",
        "import os\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transforms.Compose(transform_)\n",
        "        #self.unaligned = unaligned # ??\n",
        "\n",
        "        lst_data = os.listdir(self.data_dir)\n",
        "        self.lst_data_jazz = [f for f in lst_data if f.startswith('j')]\n",
        "        self.lst_data_pop = [f for f in lst_data if f.startswith('p')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(len(self.lst_data_jazz), len(self.lst_data_jazz))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        jazz = np.load(os.path.join(self.data_dir,self.lst_data_jazz[index]))\n",
        "        pop = np.load(os.path.join(self.data_dir,self.lst_data_pop[index]))\n",
        "        \n",
        "        if self.transform:\n",
        "            jazz = self.transform(jazz)\n",
        "            pop = self.transform(pop)\n",
        "        \n",
        "        return {'jazz': jazz, 'pop': pop}\n",
        "\n",
        "## 트렌스폼 구현하기\n",
        "class ToTensor(object):\n",
        "    def __call__(self, data):\n",
        "        for key, value in data.items():\n",
        "            value = value.transpose((2, 0, 1)).astype(np.float32)\n",
        "            data[key] = torch.from_numpy(value)\n",
        "\n",
        "        return data\n",
        "\n",
        "class Normalization(object):\n",
        "    def __init__(self, mean=0.5, std=0.5):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, data):\n",
        "        for key, value in data.items():\n",
        "            data[key] = (value - self.mean) / self.std\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "\n",
        "# DCGAN에 사용할 selanA image data가 DCGAN 모델의 generator output인 64x64와 맞지 않으므로\n",
        "# resize 해주는 transform class 선언\n",
        "class Resize(object):\n",
        "    def __init__(self,shape):\n",
        "        self.shape = shape\n",
        "\n",
        "    def __call__(self, data):\n",
        "        for key, value in data.items():\n",
        "            data[key] = resize(value, output_shape=(self.shape[0],self.shape[1],\n",
        "                                                    self.shape[2]))\n",
        "        return data\n",
        "\n",
        "class RandomCrop(object):\n",
        "  def __init__(self, shape):\n",
        "      self.shape = shape\n",
        "\n",
        "  def __call__(self, data):\n",
        "    # input, label = data['input'], data['label']\n",
        "    # h, w = input.shape[:2]\n",
        "\n",
        "    h, w = data['label'].shape[:2]\n",
        "    new_h, new_w = self.shape\n",
        "\n",
        "    top = np.random.randint(0, h - new_h)\n",
        "    left = np.random.randint(0, w - new_w)\n",
        "\n",
        "    id_y = np.arange(top, top + new_h, 1)[:, np.newaxis]\n",
        "    id_x = np.arange(left, left + new_w, 1)\n",
        "\n",
        "    # input = input[id_y, id_x]\n",
        "    # label = label[id_y, id_x]\n",
        "    # data = {'label': label, 'input': input}\n",
        "\n",
        "    # Updated at Apr 5 2020\n",
        "    for key, value in data.items():\n",
        "        data[key] = value[id_y, id_x]\n",
        "\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTWS68z4ksT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = '/content/drive/My Drive/ADV_Project_Music_style_transform/cqt_sample/other'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKVbMY-hj_hJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transforms_ = [transforms.ToTensor(),transforms.Normalize((0.5,0.5),(0.5,0.5))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id1e8NGVj40b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataloader = DataLoader(Dataset(data_dir, transform=transforms_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtEYepyem7Fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM7OXxV8lBpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, data in enumerate(dataloader):\n",
        "    real_j = data['jazz'].to(device)\n",
        "    real_p = data['pop'].to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc5okxXnlsn4",
        "colab_type": "code",
        "outputId": "7a1a4a1c-760b-4082-f3d5-6c1184589802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "real_p.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 2, 84, 938])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 336
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnOHiyW7rO8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slpQcJH_qVgD",
        "colab_type": "code",
        "outputId": "6b451571-1a96-425b-ee13-ab0376e1c5a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# Networks\n",
        "netG_A2B = Generator(2, 2).to(device)\n",
        "netG_B2A = Generator(2, 2).to(device)\n",
        "netD_A = Discriminator(2).to(device)\n",
        "netD_B = Discriminator(2).to(device)\n",
        "\n",
        "netG_A2B.apply(weights_init_normal)\n",
        "netG_B2A.apply(weights_init_normal)\n",
        "netD_A.apply(weights_init_normal)\n",
        "netD_B.apply(weights_init_normal)\n",
        "\n",
        "# Lossess\n",
        "criterion_GAN = torch.nn.MSELoss().to(device)\n",
        "criterion_cycle = torch.nn.L1Loss().to(device)\n",
        "criterion_identity = torch.nn.L1Loss().to(device)\n",
        "\n",
        "# Optimizers & LR schedulers\n",
        "optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n",
        "                                lr=opt.lr, betas=(0.5, 0.999))\n",
        "optimizer_D_A = torch.optim.Adam(netD_A.parameters(), lr=opt.lr, betas=(0.5, 0.999))\n",
        "optimizer_D_B = torch.optim.Adam(netD_B.parameters(), lr=opt.lr, betas=(0.5, 0.999))\n",
        "\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n",
        "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n",
        "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:114: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-340-0fff13fe7363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Optimizers & LR schedulers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n\u001b[0;32m---> 19\u001b[0;31m                                 lr=opt.lr, betas=(0.5, 0.999))\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0moptimizer_D_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetD_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0moptimizer_D_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetD_B\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc65d97BrHRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}